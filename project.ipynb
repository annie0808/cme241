{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Project: Optimal Asset Allocation/Consumption, Merton's Problem}$\n",
    "\n",
    "$\\textbf{Introduction:}$\n",
    "\n",
    "This project is to model Merton's Problem using RL. This problem is attractive because it is highly related to an essential topic of finance, which is the portfolio construction. \n",
    "\n",
    "$\\textbf{Merton's Problem}$\n",
    "\n",
    "First, let us recall Merton's problem with all the arguments under MDP:\n",
    "\n",
    "The State is $(t, W_t)$, the Action is $[\\pi_t,c_t]$, the Reward per unit time is $U(c_t)$, the Return is the usual accumulated discounted Reward. \n",
    "\n",
    "Goal: Find Policy : $(t, W_t) â†’ [\\pi_t,c_t]$ that maximizes the Expected Return.\n",
    "\n",
    "The wealth can be represented as $W_{t+1}=(1+r)(W_t-x_t)+(1 + R)x_t=(1+r)W_t+S_t x_t$.\n",
    "\n",
    "Then we use Bellman Optimality Equation:\n",
    "\n",
    "$V(t,W_t)=\\max_{x_t}\\gamma E \\left(V(t+1,W_{t+1})\\right)=\\max_{x_t}\\gamma E\\left(V(t+1,(1+r)W_t+S_tx_t)\\right)=\\gamma^{T-t}E\\{U(W_T)|W_t\\}=\\gamma^{T-t}E \\left(-\\frac{e^{-aW_{T}}}{a}|W_t\\right)$ using CRRA.\n",
    "\n",
    "$V(t, W_t)=-\\frac{\\gamma^{T -t}}{a}e^{-\\frac{(\\mu-r)^2(T-t)}{2\\sigma^2}-a(1+r)^{T-t}W_t}$\n",
    "\n",
    "To be more specific, we also have the following parameters: $\\gamma$: discount rate; r: risky free rate; inside CRRA equation, we have: $\\mu:$ vector of mean of risky assets; gamma: used in the utility comsumption, different from discount rate; $\\Sigma:$ covariances matrix of risky assets.\n",
    "\n",
    "$\\textbf{Methodology}$\n",
    "\n",
    "We use the function approximation as well as policy gradient for this project. For these part of code, it is inherited from our code of dunction approximation and policy gradient. Especially, for the sake of convenience, we simplify the model where we assume the return of the risky asset has normal distribution. However, we should know it is impossible in the reality. And we will use Actor-Critic Policy Gradient Algorithms with TD(0). Here we estimate $\\mu$ and $\\Sigma$ in the critic. We estimate the parameters of actions with respect to the states in the actor. Furthermore, we choose beta distribution for the policy gradient because the actions is between 0 and 1. The pseudocode is: \n",
    "\n",
    "![title](ac.png)\n",
    "\n",
    "To see the code and comments, please see project.py. \n",
    "\n",
    "$\\textbf{Discussion}$\n",
    "\n",
    "For this project, we will use a simplified model where it only has one risky asset. However, in the real world, the situation is definitely much more complicated. It should have discrete amounts of assets to hold and discrete quantities of trades. Furthermore, there should be consumption constraint since the resource is scarce in terms of funds and money. Moreover, the risk-free rate is uncertain and changing all the time. However, due to the time limitation, we are not able to discuss such complicated situations. Using the base of current RL codes, it should be able to implement in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
