{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Jan 18 Dynamic Programming}$\n",
    "\n",
    "For policy evaluation, iteration and value iteration, please see code $\\textbf{MDP.py}.$\n",
    "\n",
    "$\\textbf{1. Policy Evaluation}$\n",
    "Using the idea of Bellman Equation: \n",
    "\n",
    "$v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) (R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\pi}(s')).$\n",
    "\n",
    "$v_{k+1} = R_{\\pi}+\\gamma P_{\\pi}v_k.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get policy evaluation\n",
    "def eval_pol(self, policy: Mapping[S,Mapping[A, float]]) -> Mapping[S, float]:\n",
    "    return {j: self.get_mrp(policy).get_val_func()[i]\\ \n",
    "            for i, j in enumerate(self.nt_states_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2. Policy Iteration}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "def iter_pol(self, policy: Mapping[S,Mapping[A, float]]) -> Tuple[Mapping[S,Mapping[A, float]], Mapping[S, float]]:\n",
    "    policy = None\n",
    "    new_p = policy\n",
    "    count = 0\n",
    "\n",
    "    while count<=100 and policy != new_p:\n",
    "\n",
    "        # update policy and value function\n",
    "        count+=1\n",
    "        policy = new_p\n",
    "        value = self.val_pol(policy)\n",
    "\n",
    "        # start from the terminal states to backward states\n",
    "        for s in self.sink_states:\n",
    "            value[s] = [s for s in self.s_r[s].values()][0]\n",
    "        new_policy = {}\n",
    "\n",
    "        # start from the rest states which are non-terminal\n",
    "        for s in self.nt_states_list:\n",
    "            sa_r = self.s_r[s]\n",
    "            trans = self.transitions[s]\n",
    "            a_v = {}\n",
    "\n",
    "            # using the definition \n",
    "            for a in trans.keys():\n",
    "                a_v[a] = self.gamma*sum([trans[a][s_prime]*value[s_prime] for s_prime in trans[a].keys()])\n",
    "            sa_q = {a: sa_r[a] + a_v[ac] for ac in trans.keys()}       \n",
    "            new_p[s] = {max(sa_q.items(),key=lambda k:k[1])[0]:1}\n",
    "\n",
    "        # update sink state\n",
    "        for s in self.sink_states:\n",
    "            new_p[s] = policy[s]\n",
    "\n",
    "\n",
    "    if pol_new != pol:\n",
    "        print(\"Fail to converge\")\n",
    "\n",
    "    return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{3. Value Iteration}$\n",
    "\n",
    "It is to find the optimal policy $\\pi$. The algorithm uses the idea of optimality of Bellman Equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_val(self) -> Tuple[Mapping[S,Mapping[A, float]], Mapping[S, float]]:\n",
    "    value = {s: 0 for s in self.states}\n",
    "    new_v = value\n",
    "    count = 0\n",
    "    policy = {}\n",
    "\n",
    "    while count <= 100 and thres>1e-8:\n",
    "        count = count+1\n",
    "\n",
    "        for s in self.states:\n",
    "            sa_q = {}\n",
    "            for a in self.transitions[s].keys():\n",
    "                trans = self.transitions[s][a]\n",
    "                sa_q[a] = self.s_r[s][a]+self.gamma*sum([trans[s_prime]*value[s_prime] for s_prime in trans.keys()])\n",
    "            new_val[s] = (max(q_sa.items(), key=lambda k: k[1]))[1]\n",
    "            policy[s] = {max(q_sa.items(), key=lambda k: k[1])[0]: 1}\n",
    "        thres = max([np.abs(new_v[s] - value[s]) for s in self.states])\n",
    "\n",
    "        # update value\n",
    "        value = new_val.copy()\n",
    "\n",
    "    if thres >1e-8:\n",
    "        print(\"Fail to converge\")            \n",
    "\n",
    "    return policy, value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
