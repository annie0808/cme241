{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Jan 18 Dynamic Programming}$\n",
    "\n",
    "For policy evaluation, iteration and value iteration, please see code $\\textbf{MDP.py}.$\n",
    "\n",
    "$\\textbf{1. Policy Evaluation}$\n",
    "Using the idea of Bellman Equation: \n",
    "\n",
    "$v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) (R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\pi}(s')).$\n",
    "\n",
    "$v_{k+1} = R_{\\pi}+\\gamma P_{\\pi}v_k.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-35a5a06949e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get policy evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0meval_pol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mrp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_val_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_states_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Mapping' is not defined"
     ]
    }
   ],
   "source": [
    "# Get policy evaluation\n",
    "def eval_pol(self, policy: Mapping[S,Mapping[A, float]]) -> Mapping[S, float]:\n",
    "    return {j: self.get_mrp(policy).get_val_func()[i]\\ \n",
    "            for i, j in enumerate(self.nt_states_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2. Policy Iteration}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "def iter_pol(self, policy: Mapping[S,Mapping[A, float]]) -> Tuple[Mapping[S,Mapping[A, float]], Mapping[S, float]]:\n",
    "    policy = None\n",
    "    new_p = policy\n",
    "    count = 0\n",
    "\n",
    "    while count<=100 and policy != new_p:\n",
    "\n",
    "        # update policy and value function\n",
    "        count+=1\n",
    "        policy = new_p\n",
    "        value = self.val_pol(policy)\n",
    "\n",
    "        # start from the terminal states to backward states\n",
    "        for s in self.sink_states:\n",
    "            value[s] = [s for s in self.s_r[s].values()][0]\n",
    "        new_policy = {}\n",
    "\n",
    "        # start from the rest states which are non-terminal\n",
    "        for s in self.nt_states_list:\n",
    "            sa_r = self.s_r[s]\n",
    "            trans = self.transitions[s]\n",
    "            a_v = {}\n",
    "\n",
    "            # using the definition \n",
    "            for a in trans.keys():\n",
    "                a_v[a] = self.gamma*sum([trans[a][s_prime]*value[s_prime] for s_prime in trans[a].keys()])\n",
    "            sa_q = {a: sa_r[a] + a_v[ac] for ac in trans.keys()}       \n",
    "            new_p[s] = {max(sa_q.items(),key=lambda k:k[1])[0]:1}\n",
    "\n",
    "        # update sink state\n",
    "        for s in self.sink_states:\n",
    "            new_p[s] = policy[s]\n",
    "\n",
    "\n",
    "    if pol_new != pol:\n",
    "        print(\"Fail to converge\")\n",
    "\n",
    "    return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{3. Value Iteration}$\n",
    "\n",
    "It is to find the optimal policy $\\pi$. The algorithm uses the idea of optimality of Bellman Equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iter_val(self) -> Tuple[Mapping[S,Mapping[A, float]], Mapping[S, float]]:\n",
    "    value = {s: 0 for s in self.states}\n",
    "    new_v = value\n",
    "    count = 0\n",
    "    policy = {}\n",
    "\n",
    "    while count <= 100 and thres>1e-8:\n",
    "        count = count+1\n",
    "\n",
    "        for s in self.states:\n",
    "            sa_q = {}\n",
    "            for a in self.transitions[s].keys():\n",
    "                trans = self.transitions[s][a]\n",
    "                sa_q[a] = self.s_r[s][a]+self.gamma*sum([trans[s_prime]*value[s_prime] for s_prime in trans.keys()])\n",
    "            new_val[s] = (max(q_sa.items(), key=lambda k: k[1]))[1]\n",
    "            policy[s] = {max(q_sa.items(), key=lambda k: k[1])[0]: 1}\n",
    "        thres = max([np.abs(new_v[s] - value[s]) for s in self.states])\n",
    "\n",
    "        # update value\n",
    "        value = new_val.copy()\n",
    "\n",
    "    if thres >1e-8:\n",
    "        print(\"Fail to converge\")            \n",
    "\n",
    "    return policy, value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
