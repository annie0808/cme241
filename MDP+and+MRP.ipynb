{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Markov Process Representation\n",
    "class MP:\n",
    "    # Initialization of MP\n",
    "    # Input: state: set, \n",
    "    #        transition probability matrix: dictionary of dictionary of float\n",
    "    def __init__(self, state, transition):\n",
    "        self.state = state\n",
    "        self.transition = transition\n",
    "\n",
    "    # Set transition probability: P[s][s'] = P[S_t+1 = s'| S_t = s]\n",
    "    def set_transition(self, s, s_prime, p_ss_prime)\n",
    "        assert(s in state and s_prime in state), 'Wrong states'\n",
    "        self.transition[s][s_prime]=p_ss_prime              \n",
    "            \n",
    "    # Get transition probability\n",
    "    def get_transition(self, s, s_prime):\n",
    "        assert(s in state and s_prime in state), 'Wrong states'\n",
    "        return self.transition[s][s_prime]\n",
    "    \n",
    "    # Check the validity of transition probability \n",
    "    # The row sum of transition matrix should be 1\n",
    "    def validation(self):  \n",
    "        for s in state:\n",
    "            p_s=0.0\n",
    "            for s_prime in state:\n",
    "                p_s = p_s + self.transition[s][s_prime]\n",
    "            assert(p_s==1), 'Invalid transition probability matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Markov Reward Process\n",
    "# Add on data structures based on MP\n",
    "class MRP(MP):\n",
    "    \n",
    "    # Initialization of MRP \n",
    "    # Input: state: set\n",
    "    #        transition: dictionary of dictionary of float\n",
    "    #        reward: dictionary of dictionary of float\n",
    "    #        gamma (discounted rate): float\n",
    "    def __init__(self, state, transition, R, gamma):\n",
    "        super.__init__(state,transition)\n",
    "        self.R = R\n",
    "        self.gamma = gamma\n",
    "        self.R_s={}\n",
    "\n",
    "    # Set reward: R[s][s']\n",
    "    def set_reward(self, s, s_prime, r_ss_prime):\n",
    "        assert(s in state and s_prime in state), 'Wrong states'\n",
    "        self.R[s][s_prime] = r_ss_prime             \n",
    "    \n",
    "    # Get reward \n",
    "    def get_reward(self, s, s_prime):\n",
    "        assert(s in state and s_prime in state), 'Wrong states'        \n",
    "        return self.R[s][s_prime]           \n",
    "\n",
    "    # Calculate reward: R_s = E[R_t+1| S_t = s]\n",
    "    def compute_reward(self):\n",
    "        for s in state:\n",
    "            R_t1 = 0\n",
    "            for s_prime in state:                \n",
    "                R_t1 += self.get_transition(s, s_prime)*self.get_reward(s, s_prime)\n",
    "            self.R_s[s] = R_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Markov Decision Process\n",
    "# Add on data structures based on MRP\n",
    "class MDP(MRP):\n",
    "    \n",
    "    # Initialization of MDP \n",
    "    # Input: state: set\n",
    "    #        action: set\n",
    "    #        transition: dictionary of dictionary of float\n",
    "    #        reward: dictionary of dictionary of dictionary of float\n",
    "    #        gamma (discounted rate): float \n",
    "    def __init__(self, state, action, transition, R, gamma):\n",
    "        super.__init__(state, transition, R, gamma)\n",
    "        self.action = action\n",
    "        self.reward = {{{}}}\n",
    "        self.R_s_a = {{}}\n",
    "        self.t_s_a = {{{}}}\n",
    "    \n",
    "    # Set transition probability\n",
    "    def set_t_s_a(self, s, s_prime, pi)\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        self.t_s_a[s][a][s_prime]= pi*self.get_transition(s,s_prime)       \n",
    "             \n",
    "    # Get transition probability\n",
    "    def get_t_s_a(self, s, s_prime):\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        return self.transition[s][a][s_prime]\n",
    "\n",
    "    # Set reward: R[s][a][s']\n",
    "    def set_reward_action(self, s, action, s_prime, pi):\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        self.reward[s][a][s_prime] = pi*self.get_reward(s,s_prime)\n",
    "    \n",
    "    # Get reward:\n",
    "    def get_reward_action(self, s, action, s_prime):\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        return reward[s][a][s_prime]\n",
    "    \n",
    "    # Calculate R(s,a) = \\sum_{s'} p(s,s',a) * r(s,s',a) \n",
    "    def compute_R_s_a(self):\n",
    "        for s in state:\n",
    "            for a in action:\n",
    "                R_t1 = 0\n",
    "                for s_prime in state:\n",
    "                    R_t1 += self.get_t_s_a(s, a, s_prime)*self.get_reward(s, a, s_prime)\n",
    "                self.R_s_a[s][a] = R_t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
