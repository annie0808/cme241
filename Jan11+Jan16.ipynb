{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Jan 11 Markov Process/Markov Reward Process}$\n",
    "\n",
    "$\\textbf{1. Markov Process}$\n",
    "\n",
    "$\\textbf{Markov process}$ is a random process where its current state is only determined by the most recent states. It has a finite set of states $S$ and has a transition probability matrix.\n",
    "\n",
    "A state $S_t$ is Markov if and only if $P(S_{t+1}|S_{t})=P(S_{t+1}|S_{1}, S_{2}, ..., S_{t}).$\n",
    "\n",
    "For the transition probability matrix, the entry $P_{ss'}$ is defined as the proability that state $s$ will jump to state $s'$. Thus $P_{ss'}=P(S_{t+1}|S_{t})$.\n",
    "\n",
    "Below is the old version for MP. Please see $\\textbf{mp.py}$ for updated code design with typeVar. Please notice that MRP is coded based on the data structure of MP. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from typing import Mapping, Set, Generic, Sequence\n",
    "from utils.generic_typevars import S\n",
    "\n",
    "# Markov Process Representation\n",
    "class MP:\n",
    "    # Initialization of MP\n",
    "    # Input: state: set, \n",
    "    #        transition probability matrix: dictionary of dictionary of float\n",
    "    def __init__(self,transition: Mapping[S, Mapping[S, float]]) -> None:\n",
    "        self.transition = transition\n",
    "        self.states = self.get_states()\n",
    "        self.transition_matrix = self.set_transition(self.transitions)\n",
    "\n",
    "    # Set transition probability: P[s][s'] = P[S_t+1 = s'| S_t = s]\n",
    "    def set_transition(self, s, s_prime, p_ss_prime)\n",
    "        assert(s in state and s_prime in state), 'Wrong states'\n",
    "        self.transition[s][s_prime]=p_ss_prime              \n",
    "            \n",
    "    # Get transition probability\n",
    "    def get_transition(self, s, s_prime):\n",
    "        assert(s in state and s_prime in state), 'Wrong states'\n",
    "        return self.transition[s][s_prime]\n",
    "    \n",
    "    # Check the validity of transition probability \n",
    "    # The row sum of transition matrix should be 1\n",
    "    def validation(self):  \n",
    "        for s in state:\n",
    "            p_s=0.0\n",
    "            for s_prime in state:\n",
    "                p_s = p_s + self.transition[s][s_prime]\n",
    "            assert(p_s==1), 'Invalid transition probability matrix'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{2. Markov Reward Process}$\n",
    "\n",
    "$\\textbf{Markov Reward process}$ is a Markobv chain with values, where it has a finite set of states $S$, a state transition probability matrix, a reward function $R$ and a discount factor $\\gamma$.\n",
    "\n",
    "Reward function definition 1: $R_{ss'} = R(S_{t}=s, S_{t+1}=s')$.} It is the reward function from the current state $s$ to its successor state $s'$. \n",
    "\n",
    "Reward function definition 2: According to David Silver, it is the conditional expectation of the random variable $R_{t+1}$ at $S_t=s$. Thus $R_{s} = E[R_{t+1}|S_{t}=s$.\n",
    "\n",
    "Furthermore, the second definition can be converted to the first definition. $R_{s} = \\sum_{s'}P_{ss'}R_{ss'}$. For this part, you are welcome to see the code $\\textbf{mrp_refined.py}.$\n",
    "\n",
    "Below is the old version for MRP. Please see $\\textbf{mrp.py}$ for updated code design with typeVar. Please notice that MRP is coded based on the data structure of MP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Markov Reward Process\n",
    "# Add on data structures based on MP\n",
    "class MRP(MP):\n",
    "    \n",
    "    # Initialization of MRP \n",
    "    # Input: state: set\n",
    "    #        transition: dictionary of dictionary of float\n",
    "    #        reward: dictionary of dictionary of float\n",
    "    #        gamma (discounted rate): float\n",
    "    def __init__(self, state, transition, R, gamma):\n",
    "        super.__init__(state,transition)\n",
    "        self.R = R\n",
    "        self.gamma = gamma\n",
    "        self.R_s={}\n",
    "\n",
    "    # Set reward: R[s][s']\n",
    "    def set_reward(self, s, s_prime, r_ss_prime):\n",
    "        assert(s in state and s_prime in state), 'Wrong states'\n",
    "        self.R[s][s_prime] = r_ss_prime             \n",
    "    \n",
    "    # Get reward \n",
    "    def get_reward(self, s, s_prime):\n",
    "        assert(s in state and s_prime in state), 'Wrong states'        \n",
    "        return self.R[s][s_prime]           \n",
    "\n",
    "    # Calculate reward: R_s = E[R_t+1| S_t = s]\n",
    "    def compute_reward(self):\n",
    "        for s in state:\n",
    "            R_t1 = 0\n",
    "            for s_prime in state:                \n",
    "                R_t1 += self.get_transition(s, s_prime)*self.get_reward(s, s_prime)\n",
    "            self.R_s[s] = R_t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Jan 16 Markov Reward Process/Markov Decision Process}$\n",
    "\n",
    "$\\textbf{1. Markov Reward Process}$\n",
    "\n",
    "MRP has value function: $v(s) = E[G_t|S_t = s].$ It is the expected return at state $s$.\n",
    "\n",
    "Bellman Equation:\n",
    "\\begin{align}\n",
    "v(s) &= E[G_t|S_t = s] \\\\\n",
    "&= E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...|S_t = S]\\\\\n",
    "&= E[R_{t+1}+\\gamma G_{t+1}|S_t = S]\\\\\n",
    "& = E[R_{t+1}|S_t = S] + \\gamma E[v(S_{t+1})|S_t = s]\\\\\n",
    "& = R_s + \\gamma \\sum_{s' \\in S} P_{ss'}v(s')\n",
    "\\end{align}\n",
    "\n",
    "In the matrix form, it is $v=R+\\gamma Pv$. So it can be solved as: $v=(I-\\gamma P)^{-1}R$\n",
    "\n",
    "$\\textbf{2. Markov Decision Process}$\n",
    "\n",
    "$\\textbf{Markov decision process}$ is a Markov reward process with different actions. So it has a set of finite states $S$, a set of finite actions $A$, a state transition probability, a reward function, and discount factor $\\gamma$.\n",
    "\n",
    "$P_{ss'}^a = P(S_{t+1}=s'|S_t = s, A_t = a).$\n",
    "\n",
    "$R_s^a = E[R_{t+1}|S_t=s,A_t=a].$\n",
    "\n",
    "For actions, it has a distribution $\\pi$: $\\pi(a|s) = P(A_t=a|S_t=s).$\n",
    "\n",
    "MRP can be extracted from MDP based on the policy:\n",
    "\n",
    "$P_{ss'}^{\\pi} = \\sum_{a \\in A} \\pi(a|s)P_{ss'}^a.$\n",
    "\n",
    "$R_s^{\\pi} = \\sum_{a \\in A} \\pi(a|s)R_{s}^a.$\n",
    "\n",
    "Similar to MRP, MDP's value function also has two definitions: \n",
    "\n",
    "1. state-value: $v_\\pi(s) = E_{\\pi}[G_t|S_t=s].$\n",
    "2. action-value: $q_\\pi(s,a) = E_{\\pi}[G_t|S_t=s,A_t=a].$\n",
    "\n",
    "$\\textbf{3. Bellman Equations}$\n",
    "1. $v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) q_\\pi(s,a).$\n",
    "\n",
    "2. $q_\\pi(s,a) = R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\pi}(s').$\n",
    "\n",
    "3. $v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) (R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{\\pi}(s')).$\n",
    "\n",
    "4. $q_\\pi(s,a) = R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a \\sum_{a' \\in A} \\pi(a'|s) q_\\pi(s,a')\n",
    "\n",
    "Optimality: \n",
    "\n",
    "5. $v_*(s) = max_a q_*(s,a).$\n",
    "\n",
    "6. $q_*(s,a) = R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{*}(s')$\n",
    "\n",
    "7. $v_*(s) = max_a R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a v_{*}(s').$\n",
    "\n",
    "8. $q_*(s,a) = R_{s}^a+\\gamma \\sum_{s' \\in S} P_{ss'}^a max_a q_*(s,a).$\n",
    "\n",
    "Below is the code design of MDP. It is based on the data structure of MDP. To see updated typeVar version, please see $\\textbf{MDP.py}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Markov Decision Process\n",
    "# Add on data structures based on MRP\n",
    "class MDP(MRP):\n",
    "    \n",
    "    # Initialization of MDP \n",
    "    # Input: state: set\n",
    "    #        action: set\n",
    "    #        transition: dictionary of dictionary of float\n",
    "    #        reward: dictionary of dictionary of dictionary of float\n",
    "    #        gamma (discounted rate): float \n",
    "    def __init__(self, state, action, transition, R, gamma):\n",
    "        super.__init__(state, transition, R, gamma)\n",
    "        self.action = action\n",
    "        self.reward = {{{}}}\n",
    "        self.R_s_a = {{}}\n",
    "        self.t_s_a = {{{}}}\n",
    "    \n",
    "    # Set transition probability\n",
    "    def set_t_s_a(self, s, s_prime, pi)\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        self.t_s_a[s][a][s_prime]= pi*self.get_transition(s,s_prime)       \n",
    "             \n",
    "    # Get transition probability\n",
    "    def get_t_s_a(self, s, s_prime):\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        return self.transition[s][a][s_prime]\n",
    "\n",
    "    # Set reward: R[s][a][s']\n",
    "    def set_reward_action(self, s, action, s_prime, pi):\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        self.reward[s][a][s_prime] = pi*self.get_reward(s,s_prime)\n",
    "    \n",
    "    # Get reward:\n",
    "    def get_reward_action(self, s, action, s_prime):\n",
    "        assert(s in state and s_prime in state and a in action), 'Wrong states or action'\n",
    "        return reward[s][a][s_prime]\n",
    "    \n",
    "    # Calculate R(s,a) = \\sum_{s'} p(s,s',a) * r(s,s',a) \n",
    "    def compute_R_s_a(self):\n",
    "        for s in state:\n",
    "            for a in action:\n",
    "                R_t1 = 0\n",
    "                for s_prime in state:\n",
    "                    R_t1 += self.get_t_s_a(s, a, s_prime)*self.get_reward(s, a, s_prime)\n",
    "                self.R_s_a[s][a] = R_t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
